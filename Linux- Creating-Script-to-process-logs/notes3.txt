Part 1

No nosso script de processamento de logs, separamos em um arquivo de logs filtrados as informações sobre erros e dados sensíveis. No entanto, ao adicionar essas informações, os dados ficaram desordenados em relação à data.

É importante manter essa ordenação cronológica para termos uma visão temporal do que ocorre na aplicação, o que nos ajudará a identificar padrões, como picos de erros específicos em determinados horários ou datas, e facilitará futuras análises.

Ordenando informações com sort
Vamos ordenar o arquivo. No terminal, acessaremos a pasta com os logs usando o comando cd myapp/logs. Podemos visualizar o arquivo de logs filtrados com cat myapp-backend.log.filtrado e perceber que as datas estão desordenadas.

Por exemplo, a data 2024-09-01 aparece no início, 2024-09-03 no meio e 2024-09-02 no final. Não queremos que o arquivo permaneça assim. Para ordenar no Linux, utilizamos o comando sort, que organiza dados de um arquivo. No terminal, digitaremos:

sort myapp-backend.log.filtrado
Copiar código
A saída agora mostra os dados ordenados por data e horário.

O comando sort é simples e poderoso, ordenando por ordem alfabética, do menor para o maior. Ele possui várias opções para personalizar seu comportamento, conforme necessário.

Por exemplo, podemos inverter a ordem para que a data mais recente apareça primeiro, usando a opção -r (reverse):

sort -r myapp-backend.log.filtrado
Copiar código
A saída agora mostra os dados da data mais recente para a mais antiga.

É importante notar que o sort não altera o arquivo original, apenas a saída. Para salvar os dados ordenados em um arquivo, usamos a opção -o seguida do nome do arquivo em que queremos salvar:

sort myapp-backend.log.filtrado -o logs-ordenados
Copiar código
Após executar o comando, um arquivo com os dados ordenados será criado. Rodando ls, verificamos que o arquivo logs-ordenados existe na pasta, então podemos usar o cat para checar seu conteúdo:

cat logs-ordenados
Copiar código
Assim, temos nossas informações salvas e ordenadas como precisamos.

Incrementando o script de monitoramento
Portanto, vamos inserir esse comando em nosso script. Após abrir o script monitoramento-logs.sh com o Vim, vamos habilitar a edição.

Antes do comando done, podemos inserir o comando sort com a opção -o, substituindo myapp-backend.log.filtrado e logs-ordenados pela variável ${arquivo}.filtrado:

#!/bin/bash

LOG_DIR="../myapp/logs"

echo "Verificando logs no diretorio $LOG_DIR"

find $LOG_DIR -name "*.log" -print0 | while IFS= read -r -d '' arquivo; do
    grep "ERROR" "$arquivo" > "${arquivo}.filtrado"
    grep "SENSITIVE_DATA" "$arquivo" >> "${arquivo}.filtrado"

    sed -i 's/User password is .*/User password is REDACTED/g' "${arquivo}.filtrado"
    sed -i 's/User password reset request with token .*/User password reset request with token REDACTED/g' "${arquivo}.filtrado"
    sed -i 's/API key leaked: .*/API key leaked: REDACTED/g' "${arquivo}.filtrado"
    sed -i 's/User credit card last four digits: .*/User credit card last four digits: REDACTED/g' "${arquivo}.filtrado"
    sed -i 's/User session initiated with token: .*/User session initiated with token: REDACTED/g' "${arquivo}.filtrado"

    sort "${arquivo}.filtrado" -o "${arquivo}.filtrado"
done
Copiar código
Dessa forma, o sort sobrescreve o conteúdo do arquivo de logs filtrados com os dados ordenados.

Vamos salvar as alterações no Vim, pressionando "Esc" e depois :wq. Em seguida, executaremos o script com:

./monitoramento-logs.sh
Copiar código
Após acessar novamente a pasta de logs, verificaremos com cat myapp-backend.log.filtrado que o arquivo está ordenado por data.

Aprendemos a ordenar arquivos com sort, o que nos proporciona uma visão temporal da aplicação e auxilia nas modificações futuras no código.

-------------------------------
Part 2

Nós ordenamos o arquivo de logs filtrados, conforme a data. Porém, ao criar esse arquivo e adicionar tanto linhas de logs de erro quanto de dados sensíveis, algumas linhas ficaram duplicadas, pois os dados sensíveis, às vezes, são mensagens de erro também.

No terminal, na pasta dos logs, podemos verificar isso ao executar o comando cat myapp.backend.log.filtrado. As duas últimas linhas desse arquivo são idênticas, com a mesma data, horário e mensagem:

2024-09-03 12:00:00 ERROR: SENSITIVE_DATA: Database backup contains sensitive information.

2024-09-03 12:00:00 ERROR: SENSITIVE_DATA: Database backup contains sensitive information.

Isso aconteceu devido à presença das palavras "SENSITIVE_DATA" e "ERROR" na mesma linha dos logs.

Nosso objetivo é remover essas duplicatas para obter um arquivo mais limpo, facilitando a análise.

Removendo duplicatas com uniq
Para remover duplicatas no Linux, utilizamos o comando uniq:

uniq myapp-backend.log.filtrado
Copiar código
A saída do arquivo exibe as informações sem duplicatas. Notaremos que a última linha não está mais repetida, aparecendo apenas uma vez.

Para o uniq funcionar corretamente, é necessário que o arquivo esteja ordenado. Na aula passada, nós o ordenamos por data. Caso ele não estivesse ordenado, o comando não funcionaria corretamente.

O uniq procura duplicatas no arquivo e mantém apenas a primeira ocorrência de cada linha. Portanto, as linhas precisam estar consecutivas no arquivo para ele manter apenas uma de cada.

Além disso, é importante lembrar que o uniq não altera o arquivo diretamente. Ele faz as alterações apenas na saída do terminal. Se executarmos cat myapp-backend.log.filtrado novamente, ainda veremos as duplicatas.

Para salvar as informações sem duplicatas, podemos usar o operador de redirecionamento ">" seguido do nome do arquivo em que vamos guardar as informações:

uniq myapp-backend.log.filtrado > logs-sem-duplicatas
Copiar código
Assim, criamos um novo arquivo com os logs filtrados sem informações repetidas. Podemos executar o cat logs-sem-duplicatas para nos certificar.

Incrementando o script de monitoramento
No final do arquivo monitoramento-logs.sh, após o sort, vamos inserir o comando uniq. Substituiremos myapp-backend.log.filtrado pelo nome da variável "$arquivo.filtrado". Após o operador de redirecionamento, criaremos um arquivo chamado ${arquivo}.unico, indicando que os logs já foram filtrados e não possuem duplicatas:

#!/bin/bash

LOG_DIR="../myapp/logs"

echo "Verificando logs no diretorio $LOG_DIR"

find $LOG_DIR -name "*.log" -print0 | while IFS= read -r -d '' arquivo; do
    grep "ERROR" "$arquivo" > "${arquivo}.filtrado"
    grep "SENSITIVE_DATA" "$arquivo" >> "${arquivo}.filtrado"

    sed -i 's/User password is .*/User password is REDACTED/g' "${arquivo}.filtrado"
    sed -i 's/User password reset request with token .*/User password reset request with token REDACTED/g' "${arquivo}.filtrado"
    sed -i 's/API key leaked: .*/API key leaked: REDACTED/g' "${arquivo}.filtrado"
    sed -i 's/User credit card last four digits: .*/User credit card last four digits: REDACTED/g' "${arquivo}.filtrado"
    sed -i 's/User session initiated with token: .*/User session initiated with token: REDACTED/g' "${arquivo}.filtrado"

    sort "${arquivo}.filtrado" -o "${arquivo}.filtrado"

    uniq "${arquivo}.filtrado" > "${arquivo}.unico"
done
Copiar código
Após salvar as alterações e sair do Vim, vamos executar o script. Na pasta de logs, vamos listar os arquivos com ls. Agora, temos arquivos com a extensão .unico.

Ao executar cat myapp-backend.log.único, notaremos que os logs de erro e os logs dados sensíveis estão ordenados por data e sem duplicatas.

Na sequência, aprenderemos alguns comandos úteis para visualizar o conteúdo de arquivos no terminal

---------------------------------
Part 3
